{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c68bf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn  # All neural network modules, nn.Linear, nn.Conv2d, BatchNorm, Loss functions\n",
    "import torch.optim as optim  # For all Optimization algorithms, SGD, Adam, etc.\n",
    "import torch.nn.functional as F  # All functions that don't have any parameters\n",
    "import torchvision.datasets as datasets  # Has standard datasets we can import in a nice way\n",
    "import torchvision.transforms as transforms  # Transformations we can perform on our dataset\n",
    "from torch.utils.data import DataLoader# Gives easier dataset managment and creates mini batches\n",
    "from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06f6c944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple CNN\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_channels=1, num_classes=10):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=in_channels, out_channels=8, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "        self.fc1 = nn.Linear(16 * 7 * 7, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfbdce47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4737ecf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "in_channels = 1\n",
    "num_classes = 10\n",
    "#batch_sizes = 64\n",
    "#learning_rates = 0.001\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a1b55f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "train_dataset = datasets.MNIST(\n",
    "    root=\"dataset/\", train=True, transform=transforms.ToTensor(), download=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d6c44c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1097820",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2d1241e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc1): Linear(in_features=784, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize network\n",
    "model = CNN(in_channels=in_channels, num_classes=num_classes)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3164d26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63b4da6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [1, 64, 128, 1024]\n",
    "learning_rates = [0.1, 0.01, 0.001, 0.0001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23bdeae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss this epoch was 2.3805272048374015\n",
      "Mean loss this epoch was 2.3089076160132884\n",
      "Mean loss this epoch was 2.3022852302471795\n",
      "Mean loss this epoch was 2.3014613709688185\n",
      "Mean loss this epoch was 2.310259590270931\n",
      "Mean loss this epoch was 2.3024020472060895\n",
      "Mean loss this epoch was 2.3014148943968165\n",
      "Mean loss this epoch was 2.3012059255957857\n",
      "Mean loss this epoch was 2.3073666446498717\n",
      "Mean loss this epoch was 2.3021506576904103\n",
      "Mean loss this epoch was 2.3015295483156053\n",
      "Mean loss this epoch was 2.3011881232515834\n",
      "Mean loss this epoch was 2.3035693209050065\n",
      "Mean loss this epoch was 2.301566560389632\n",
      "Mean loss this epoch was 2.301299959926282\n",
      "Mean loss this epoch was 2.3012220697887873\n"
     ]
    }
   ],
   "source": [
    "for batch_size in batch_sizes:\n",
    "    for learning_rate in learning_rates:\n",
    "        step = 0 \n",
    "        writer = SummaryWriter(f\"runs/MNIST/MiniBatchSize {batch_size} LR {learning_rate}\")\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.0)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            losses = []\n",
    "            accuracies = []\n",
    "\n",
    "        for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "            # Get data to cuda if possible\n",
    "            data = data.to(device=device)\n",
    "            targets = targets.to(device=device)\n",
    "\n",
    "            # forward\n",
    "            scores = model(data)\n",
    "            loss = criterion(scores, targets)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            # backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate 'running' training accuracy\n",
    "            _, predictions = scores.max(1)\n",
    "            num_correct = (predictions == targets).sum()\n",
    "            running_train_acc = float(num_correct) / float(data.shape[0])\n",
    "\n",
    "            # Plot things to tensorboard\n",
    "            writer.add_scalar(\"Training loss\", loss, global_step=step)\n",
    "            writer.add_scalar(\"Training Accuracy\", running_train_acc, global_step=step)\n",
    "            step += 1\n",
    "\n",
    "        print(f'Mean loss this epoch was {sum(losses)/len(losses)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5769fed0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a90111",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa05530",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f06a40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
